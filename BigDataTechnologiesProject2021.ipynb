{
  "metadata": {
    "name": "BigDataTechnologiesProject2021template",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Spark Project\n#### Exploring the Ling-Spam email dataset\n\n**Language**: Scala\n\n**Authors**: Lynda"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "To run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In the following paragraphs we are going to execute Spark code, run shell commands to download and move files, run sql queries etc. Each paragraph will start with `%` followed by an interpreter name, e.g. `%spark` for a Spark interpreter. Different interpreter names indicate what will be executed: code, markdown, html etc.  This allows you to perform data ingestion, munging, wrangling, visualization, analysis, processing and more, all in one place!\n\nThroughtout this notebook we will use the following interpreters:\n\n- `%spark` - Spark interpreter to run Spark code written in Scala\n- `%md` - Markdown for displaying formatted text, links, and images\n\nTo learn more about Zeppelin interpreters check out this [link](https://zeppelin.apache.org/)."
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nspark.version"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.RDD"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sc.wholeTextFiles(\"/tmp/ling-spam/spam/spmsgc99.txt\").collect"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\ndef probaWordDir(sc:SparkContext)(filesDir:String)\n  :(RDD[(String, Double)], Long) \u003d {\n    // reading all the text files withing filesDir\n    // pair : (name of the input file, text)\n    val rdd \u003d sc.wholeTextFiles(filesDir)\n    \n    // number of files\n    val nbFiles \u003d rdd.count()\n    \n    // list of non-informative words\n    val nonInformativeWords \u003d List(\".\", \":\", \",\", \" \", \"/\",\"\\\\\", \"-\", \"\u0027\", \"(\", \")\", \"@\")\n                                 \n    \n    // RDD where the key is the file name and the value is the list of unique words contained in it\n    // steps à appliquer sur value : \n    // 1) remove extra spaces using : trim()\n    // 2) split (separated by \"\\\\s+\")\n    // 3) keep only informative words, to do that use filterNot\n    \n    val filesWords: RDD[(String, List[String])]\u003d rdd.map(textFile\u003d\u003e(textFile._1,textFile._2.trim().split(\"\\\\s+\").distinct.toList.filterNot(nonInformativeWords.toSet)))\n    \n    // number of occurences of each word among all files\n    // for each file (in value list) of a word container(key) we assign 1, will gonna get a list of pair (file,1)\n    // then we use reduceByKey to sum over the files\n    val wordDirOccurency: RDD[(String,Int)] \u003d filesWords.flatMap(w\u003d\u003ew._2.map(f\u003d\u003e(f,1))).reduceByKey(_+_)\n    \n    //probability \n    // proba \u003d ratio \u003d number of occurrences of the word / nbFiles\n    // !!! convert to double to not have problems with the rest of the division !!!\n    val probaWord: RDD[(String,Double)] \u003d wordDirOccurency.map(wo\u003d\u003e(wo._1,wo._2.toDouble/nbFiles))\n    \n    (probaWord, nbFiles) // couple returned by the function\n    \n }\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval rdd \u003d sc.wholeTextFiles(\"/tmp/ling-spam/spam/spmsgc99.txt\")\nval nonInformativeWords \u003d List(\".\", \":\", \",\", \" \", \"/\",\"\\\\\", \"-\", \"\u0027\", \"(\", \")\", \"@\")\nval rddsplit \u003d rdd.map(textFile\u003d\u003e(textFile._1,textFile._2.trim().split(\"\\\\s+\").distinct) )\nval rddsplit2 \u003d rdd.map(textFile\u003d\u003e(textFile._1,textFile._2.split(\"\\\\s+\").distinct) )\nrddsplit.collect\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nrddsplit2.collect"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n\ndef computeMutualInformationFactor(\n    probaWC:RDD[(String, Double)],// probability the word occurs (or not) in an email of a given class(spam or ham)\n    probaW:RDD[(String, Double)], // probability the word occurs (whatever the class)\n    probaC: Double, // probability that an email belongs to the given class, p(class)\n    default: Double // default value when a probability is missing\n):RDD[(String, Double)] \u003d {\n    // get RDD where \n    // key \u003d word\n    // value (probability the word occurs, probability the word occurs to a specific class)\n    // i.e : (word,(p(occurs),p(occurs,class))\n    // use leftOuterJoin to get none if no proba found\n    val probWJoin: RDD[(String, (Double, Option[Double]))] \u003d probaW.leftOuterJoin(probaWC)\n    \n    // same as the one above, however\n    // here we replace when the probability probaWC is missing\n    // (when we have a none returned by the join) by\n    // the default proba\n    val factors: RDD[(String, (Double, Double))] \u003d probWJoin.map(wp \u003d\u003e (wp._1, (wp._2._1, wp._2._2.getOrElse(default))))\n    \n    \n    //Computing using the given formule \n    // Remark :\n    // we need log2 but we have only log \n    // we use : log2 \u003d log/log(2)\n    \n    // log2 \u003d log(p(o,c)/p(o)p(c)))/log(2)\n    // p(o,c)*log2\n    // for each word : (word,p(o,c)*log(p(o,c)/p(o)p(c))/log(2))\n    // supp wp is (word,(p(occurs),p(occurs,class))\n    // wp._1 \u003d word\n    //wp._2._1 \u003d p(occurs) \u003d p(o)\n    // wp._2._2 \u003d p(occurs,class) \u003d p(o,c)\n    factors.map(wp \u003d\u003e (wp._1, wp._2._2 * math.log(wp._2._2 / (wp._2._1 * probaC)) / math.log(2.0)))\n    \n  }\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// Read the directories containing the data\n\nval (pPresentGivenSpam, nSpam) \u003d probaWordDir(sc)(\"/tmp/ling-spam/spam/*\")\nval (pPresentGivenHam, nHam) \u003d probaWordDir(sc)(\"/tmp/ling-spam/ham/*\")\n\n\npPresentGivenSpam.collect().foreach(println)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// main part of the notebook\n  def main(args: Array[String]) {\n      // the directory  path is given as an argument to the main  !!! \n      // we should hold exception if don\u0027t have it\n      // ie : /tmp/ling-spam/\n      if(args.length\u003c\u003d0){\n          println(\"No directory path provided\")\n      }\n      val (probaW, nbFiles) \u003d probaWordDir(sc)(args(0) + \"/*/*.txt\") // p(occurs), nbFiles in total\n      // COmputing the couple (ptobaWordHam,nbFilesHam)\n      val (probaWordHam, nbFilesHam) \u003d probaWordDir(sc)(args(0).concat(\"/ham/*.txt\")) // the arg given to the param is : /tmp/ling-spam/ham/*\"\n      val (probaWordSpam, nbFilesSpam) \u003d probaWordDir(sc)(args(0).concat(\"/spam/*.txt\")) // the arg given to the param is : /tmp/ling-spam/spam/*\"\n       \n      //Computing the probability P(occurs, class) for each word.\n      // Each RDD has the map structure: word \u003d\u003e probability the word occurs (or not) in an email of a given class.\n      // p(true,ham) \u003d probaWordHam , p(false,ham),\u003d 1 - probaWordHam (probaWordHam.map(x \u003d\u003e (x._1, 1 - x._2))\n      // p(true,spam) \u003d probaWordSpam , p(false,spam) \u003d 1 - probaWordSpam  (probaWordSpam.map(x \u003d\u003e (x._1, 1 - x._2)\n     // the 4 RDDs : \n      val probaWC \u003d (probaWordHam, probaWordSpam, probaWordHam.map(x \u003d\u003e (x._1, 1 - x._2)), probaWordSpam.map(x \u003d\u003e (x._1, 1 - x._2)))\n\n    // the probability that an email belongs to the given class : p(class)\n    val probaHam \u003d nbFilesHam.toDouble / nbFiles.toDouble  // p(Ham)\n    val probaSpam \u003d nbFilesSpam.toDouble / nbFiles.toDouble // p(Spam)\n    val defaultProba \u003d 10^(-5) //default value \n    // Compute mutual information for each class and occurs \n    val MITrueHam \u003d computeMutualInformationFactor(probaWC._1, probaW, probaHam, defaultProba) \n    val MITrueSpam \u003d computeMutualInformationFactor(probaWC._2, probaW, probaSpam, defaultProba)\n    val MIFalseHam \u003d computeMutualInformationFactor(probaWC._3, probaW, probaHam, defaultProba)\n    val MIFalseSpam \u003d computeMutualInformationFactor(probaWC._4, probaW, probaSpam, defaultProba)\n    \n    \n    \n    //computing the mutual information of each word as a RDD with the map structure: word \u003d\u003e MI(word)\n    val MI :RDD[(String, Double)] \u003d MITrueHam.union(MITrueSpam).union(MIFalseHam).union(MIFalseSpam).reduceByKey( (x, y) \u003d\u003e x + y)\n    \n    \n    //  print on screen the 20 top words \n     val topWords: Array[(String, Double)] \u003d MI.top(20)(Ordering[Double].on(x \u003d\u003e x._2))\n     topWords.map(_\u003d\u003eprintln)\n     \n    //Save all the words and their mutual information values, sorted and the corresp top words must be also stored on HDFS \n    val path: String \u003d \"/tmp/topWords.txt\"\n    sc.parallelize(topWords).keys.coalesce(1, true).saveAsTextFile(path)\n    sc.wholeTextFiles(\"/tmp/topWords.txt\").collect \n\n     \n}\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%spark\nmain(Array(\"/tmp/ling-spam\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nsc.wholeTextFiles(\"/tmp/topWords.txt\").collect "
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}